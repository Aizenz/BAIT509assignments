{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAIT 509 Assignment 3: Logistic Regression and Evaluation Metrics  \n",
    "\n",
    "__Evaluates__: Lectures 6 - 9. \n",
    "\n",
    "__Rubrics__: Your solutions will be assessed primarily on the accuracy of your coding, as well as the clarity and correctness of your written responses. The MDS rubrics provide a good guide as to what is expected of you in your responses to the assignment questions and how the TAs will grade your answers. See the following links for more details:\n",
    "\n",
    "- [mechanics_rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_mech.md): submit an assignment correctly.\n",
    "- [accuracy rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md): evaluating your code.\n",
    "- [reasoning rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md): evaluating your written responses.\n",
    "- [autograde rubric](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_autograde.md): evaluating questions that are either right or wrong (can be done either manually or automatically).\n",
    "\n",
    "## Tidy Submission \n",
    "rubric={mechanics:2}\n",
    "\n",
    "- Complete this assignment by filling out this jupyter notebook.\n",
    "- Any place you see `...` or `____`, you must fill in the function, variable, or data to complete the code.\n",
    "- Use proper English, spelling, and grammar.\n",
    "- You will submit two files on Canvas:\n",
    "    1. This jupyter notebook file containing your responses ( an `.ipynb` file); and,\n",
    "    2. An `.html` file of your completed notebook that will render directly on Canvas without having to be downloaded.\n",
    "        - To generate this html file you can click `File` -> `Export Notebook As` -> `HTML` in JupyterLab or type the following into a terminal `jupyter nbconvert --to html_embed assignment.ipynb`).\n",
    "    \n",
    "Submit your assignment through UBC Canvas by the deadline listed there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and learning goals <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "Welcome to the assignment! In this assignment, you will practice:\n",
    "\n",
    "- Explain components of a confusion matrix.\n",
    "- Define precision, recall, and f1-score and use them to evaluate different classifiers.\n",
    "- Identify whether there is class imbalance and whether you need to deal with it.\n",
    "- Explain `class_weight` and use it to deal with data imbalance.\n",
    "- Apply different scoring functions with `cross_validate` and `GridSearchCV` and `RandomizedSearchCV`.\n",
    "- Explain the general intuition behind linear models.\n",
    "- Explain the `fit` and `predict` paradigm of linear models.\n",
    "- Use `scikit-learn`'s `LogisticRegression` classifier.\n",
    "    - Use `fit`, `predict` and `predict_proba`.   \n",
    "    - Use `coef_` to interpret the model weights.\n",
    "- Explain the advantages and limitations of linear classifiers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:  Precision, recall, and f1 score \"by hand\" (without `sklearn`) <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "\n",
    "Consider the problem of predicting whether a new product will be successful or not and is worth investing in. Below are confusion matrices of two machine learning models: Model A and Model B. \n",
    "\n",
    "##### Model A\n",
    "|    Actual/Predicted         | Predicted successful| Predicted not successful |\n",
    "| :-------------------------- | ------------------: | -----------------------: |\n",
    "| **Actually successful**     | 3                   | 5                        |\n",
    "| **Actually not successful** | 6                   | 96                       |\n",
    " \n",
    "\n",
    "##### Model B\n",
    "|    Actual/Predicted         | Predicted successful| Predicted not successful |\n",
    "| :-------------------------- | ------------------: | -----------------------: |\n",
    "| **Actually successful**     | 6                   |                        14 |\n",
    "| **Actually not successful** | 0                  |                       90 |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Positive vs. negative class\n",
    "rubric={autograde:1, reasoning:1}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Precision, recall, and f1 score depend crucially upon which class is considered \"positive\", that is the thing you wish to find. In the example above, which class ( `Actually successful` or `Actually not successful`)  is likely to be the \"positive\" class and why?\n",
    "\n",
    "Save the label name in a string object named `answer_1_1`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually successful is likely to be the *positive* class, because it is the target we are going to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_1_1 = 'Actually successful'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Accuracy\n",
    "rubric={autograde:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Calculate accuracies for Model A and Model B. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19893d9ec734e650",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8727272727272727"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_a_acc = (3+ 96) / (3+5+6+96)\n",
    "model_b_acc = (6+90) / (6+14+90)\n",
    "display(model_a_acc)\n",
    "display(model_b_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Which model would you pick? \n",
    "rubric={reasoning:1}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Which model would you pick simply based on the accuracy metric? \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model A - Precision, recall, f1-score\n",
    "rubric={accuracy:1.5}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Calculate precision, recall, f1-score for **Model A** by designating the appropriate fraction to objects named `a_precision`, `a_recall` and `a_f1`. \n",
    "\n",
    "You can use the objects `a_precision` and `a_recall` to use in your `a_f1` calculation.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-feae0ff2bfe786c6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.35294117647058826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_precision = 3 / 9\n",
    "a_recall = 3 / 8\n",
    "a_f1 = 2 /(1/a_precision + 1/a_recall)\n",
    "display(a_precision)\n",
    "display(a_recall)\n",
    "display(a_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Model B - Precision, recall, f1-score\n",
    "rubric={accuracy:1.5}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Calculate precision, recall, f1-score for **Model B** by designating the appropriate fraction to objects named `b_precision`, `b_recall` and `b_f1`. \n",
    "\n",
    "You can use the objects `b_precision` and `b_recall` to use in your `b_f1` calculation.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ae8130bd6cb0aa3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.46153846153846145"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_precision = 6 / 6\n",
    "b_recall = 6 / (6 + 14)\n",
    "b_f1 = 2 / (1/b_precision + 1/b_recall)\n",
    "display(b_precision)\n",
    "display(b_recall)\n",
    "display(b_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Metric choice\n",
    "rubric={reasoning:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Which metric(s) is more informative in this case? Why? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model B is more informative in this case, it shows a high dispersion between precision, recall and accuracy. The zero false negative illustrates its extremely high precison while their accuracies can not display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Model choice\n",
    "rubric={reasoning:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "\n",
    "Which model would you pick based on this information and why? \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***B***\n",
    "\n",
    "Because of the high precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Sentiment analysis on the IMDB dataset: model building <a name=\"3\"></a>\n",
    "<hr>\n",
    "\n",
    "<img src=\"https://ia.media-imdb.com/images/M/MV5BMTk3ODA4Mjc0NF5BMl5BcG5nXkFtZTgwNDc1MzQ2OTE@._V1_.png\"  width = \"40%\" alt=\"404 image\" />\n",
    "\n",
    "In this exercise, you will carry out sentiment analysis on a real corpus, [the IMDB movie review dataset](https://www.kaggle.com/utathya/imdb-review-dataset).\n",
    "The starter code below loads the data CSV file (assuming that it's in the data directory) as a pandas DataFrame called `imdb_df`.\n",
    "\n",
    "The supervised learning task is, given the text of a movie review, to predict whether the review sentiment is positive (reviewer liked the movie) or negative (reviewer disliked the movie). We have done a bit of preprocessing on the dataset already where the positive review are labelled `1` and the negative reviews are labelled `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>This movie did attempt to capture the naive idealism that many young teenaged girls have for fun, friendship, escape, danger, sex, maturity, etc. The problem was that it failed to establish these things on every single level; which is why it failed to build a decent story around them. I couldn't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>The Earth is destined to be no more thanks to Father Pergado and a bunch of Nuns. Christopher Lee (who has since said that he was duped in to appearing in this by his producers who told him loads of great actors were involved) is Father Pergado and gets to do his usual serious and scary routine....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8157</th>\n",
       "      <td>I didn't see this movie when it originally came out, but there has been a couple songs sharing the title and the term still gets used from time to time and I figured there must be something to the flick, so I dug it up and gave a view. Now I would like the approximate hour and forty five minutes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4435</th>\n",
       "      <td>I saw this film purely based on the fact that it was on the DPP Video Nasty list, and while I'm glad I saw it because it's now 'another Video Nasty down' - on its own merits, Andy Milligan's film really isn't worth bothering with. There are, of course, far worse films on the infamous list; but t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10168</th>\n",
       "      <td>Having seen only once and in the dawn hours, I can't seem to forget this haunting film. A mix of mystery, suspense, and heartbreaking romance it reminds me of Vertigo.The actors, though not that well known are good especially Joan Hackett in one of her best performances.You believe in her, in he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                            review  \\\n",
       "2139   This movie did attempt to capture the naive idealism that many young teenaged girls have for fun, friendship, escape, danger, sex, maturity, etc. The problem was that it failed to establish these things on every single level; which is why it failed to build a decent story around them. I couldn't...   \n",
       "7454   The Earth is destined to be no more thanks to Father Pergado and a bunch of Nuns. Christopher Lee (who has since said that he was duped in to appearing in this by his producers who told him loads of great actors were involved) is Father Pergado and gets to do his usual serious and scary routine....   \n",
       "8157   I didn't see this movie when it originally came out, but there has been a couple songs sharing the title and the term still gets used from time to time and I figured there must be something to the flick, so I dug it up and gave a view. Now I would like the approximate hour and forty five minutes...   \n",
       "4435   I saw this film purely based on the fact that it was on the DPP Video Nasty list, and while I'm glad I saw it because it's now 'another Video Nasty down' - on its own merits, Andy Milligan's film really isn't worth bothering with. There are, of course, far worse films on the infamous list; but t...   \n",
       "10168  Having seen only once and in the dawn hours, I can't seem to forget this haunting film. A mix of mystery, suspense, and heartbreaking romance it reminds me of Vertigo.The actors, though not that well known are good especially Joan Hackett in one of her best performances.You believe in her, in he...   \n",
       "\n",
       "       label  \n",
       "2139       0  \n",
       "7454       0  \n",
       "8157       0  \n",
       "4435       0  \n",
       "10168      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)  # Set how wide columns to show (to be able to see the reviews)\n",
    "\n",
    "imdb_df = pd.read_csv(\"imdb_speed.csv\")\n",
    "train_df, test_df = train_test_split(imdb_df, test_size=0.2, random_state=77)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature and target objects \n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Separate our feature vectors from the target.\n",
    "\n",
    "You will need to do this for both `train_df` and `test_df`.\n",
    "\n",
    "Save the results in objects named `X_train`, `y_train`, `X_test` and `y_test`. \n",
    "\n",
    "(Makes sure that all 4 of these objects are of type Pandas Series. We will be using `CountVectorizer` for future questions and this transformation requires an input of Pandas Series)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27ddb033928d3ba8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_df['review'], train_df['label'], test_df['review'], test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dummy classifier\n",
    "rubric={accuracy:3}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Make a baseline model using `DummyClassifier`.\n",
    "\n",
    "Carry out cross-validation using the `stratified` strategy. Pass the following `scoring` metrics to `cross_validate`. \n",
    "- accuracy\n",
    "- f1\n",
    "- recall\n",
    "- precision\n",
    "\n",
    "(We are using cross-validation here since we can obtain multiple scores at once) \n",
    "\n",
    "Make sure you use  `return_train_score=True` and 5-fold cross-validation.\n",
    "\n",
    "Save your results in a dataframe named `dummy_scores_df`. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03b5ef93dd1fc50d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = DummyClassifier()\n",
    "cross_validate(model, )\n",
    "\n",
    "\n",
    "dummy_scores_df = ...\n",
    "\n",
    "dummy_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dummy classifier mean\n",
    "rubric={accuracy:1}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "What is the mean of each column in `dummy_scores_df`?\n",
    "\n",
    "Save your result in an object named `dummy_mean`. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e4826f2524485549",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_mean = ...\n",
    "\n",
    "dummy_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Pipeline\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Let's make a pipeline now. \n",
    "\n",
    "Since we only have 1 column to preprocess, we only need 1 main pipeline for this question. \n",
    "\n",
    "Create a pipeline with 2 steps, one for `CountVectorizer` and one with a `LogisticRegression` model. For the LogisticRegression model, it's a good idea to set the argument `max_iter=5000` to avoid any warnings and convergence issues. Also let's balance the classes in our splits by setting the appropriate argument in `LogisticRegression` (read its docstring to find out how to achieve this). \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Hyperparameter optimization\n",
    "rubric={accuracy:4}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Perform hyperparameter turning using a random search of 10 hyperparameter combinations.\n",
    "We have provided the `params_grid`,\n",
    "which contains a distribution of each parameter using scipy distribution functions.\n",
    "In the interest of time,\n",
    "you can limit your cross-validation of the hyperparameter combinations to 3-fold\n",
    "and set the computation to run in parallel using `n_jobs`.\n",
    "Also, use verbose output and return the training score.\n",
    "Instead of the default scoring metric,\n",
    "use the f1 score.\n",
    "\n",
    "Finally,\n",
    "make sure to fit your model on the entire training dataset. \n",
    "\n",
    "This can take a few minutes so please be patient.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-907dab64153f296f",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "param_grid = {\n",
    "    \"countvectorizer__max_features\": randint(10, 10000),\n",
    "    \"logisticregression__C\": loguniform(0.01, 100),\n",
    "}\n",
    "\n",
    "random_search = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6  Best hyperparameters\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "What are the best hyperparameter values found by `RandomizedSearchCV` for `C` and `max_features`. \n",
    "What was the corresponding validation score? Output these values either in separate cells or by printing all of them from a single cell (but we need to see all to be able to grade).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-735fc79f5b175920",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_parameters = ...\n",
    "optimal_score = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Hyperparameters and the fundamental tradeoff\n",
    "rubric={reasoning:3}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Write 2-3 sentences each on the two questions below,\n",
    "You can add code if needed or use information from previously executed cells\n",
    "if you deem that to be sufficient.\n",
    "\n",
    "1. From the set of possible models in the search, did your search return a relatively simple CountVectorizer or a relatively complex one?\n",
    "2. Did it return a relatively simple LogisticRegression or a relatively complex one? Here ‘simple’ and ‘complex’ we mean with respect to the fundamental tradeoff.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any potential code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Train and test scores of best scoring model\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "What is the train and test `f1` score of the best scoring model?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Confusion matrix\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Plot a confusion matrix on the test set using your random search object as your estimator.\n",
    "Use the `display_labels` parameter so that it's easier to recognized which class is a positive review and which is negative. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Classification report\n",
    "rubric={accuracy:3}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Print a classification report on the `X_test` predictions of your random search object's best model with measurements to 4 decimal places. Use this information to answer the questions:\n",
    "\n",
    "1. What is the recall if we classify `1` as our \"positive\" class? \n",
    "2. What is the precision weighted average? Save the result to 4 decimal places. \n",
    "3. What is the `f1` score using `1` as your positive class?\n",
    "4. Comment on the overall model performance in the context of what we used as input features.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3: Model Interpretation <a name=\"4\"></a>\n",
    "<hr>\n",
    "\n",
    "One of the primary advantage of linear models is their ability to interpret models in terms of important features. In this exercise, we'll explore the weights learned by logistic regression classifier.\n",
    "\n",
    "Below we've create a dataframe that contains the words used in our optimal model along with their coefficients (if you named your `RandomizedSearchCV` object in question 2.5 something else than `random_search`, change the code below accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'words': best_estimator[ \"countvectorizer\"].get_feature_names(),\n",
    "    'coefficient': best_estimator[\"logisticregression\"].coef_[0]\n",
    "})\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get the most informative positive words\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Using the dataframe `coef_df` above, find the 10 words that are most indicative of a positive review.\n",
    "\n",
    "Elaborate on the positive words here - Do they make sense with their target value?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get the most informative negative words\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Using the dataframe `coef_df` above, find the 10 words that are most indicative of a negative review.\n",
    "\n",
    "Elaborate on the negative words here - Do they make sense with their target value?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Explaining the coefficients?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Do the words associated with positive and negative reviews make sense? Why is it useful to get access to this information?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Using `predict` vs `predict_proba`\n",
    "rubric={accuracy:3}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Make a dataframe named `results_df` that contains these 5 columns: \n",
    "\n",
    "- `review` - this should contain the reviews from `X_test`.\n",
    "- `true_label` - This should contain the true `y_test` values. \n",
    "- `predicted_y` - The predicted labels generated from `best_model` for the `X_test` reviews using `.predict()`. \n",
    "- `neg_label_prob` - The probabilities of class `0` generated from `best_model` for the `X_test` reviews. These can be found at index 0 of the `predict_proba` output (you can get that using `[:,0]`). \n",
    "-  `pos_label_prob` - The probabilities of class `1` generated from `best_model` for the `X_test` reviews. These can be found at index 0 of the `predict_proba` output (you can get that using `[:,1]`). \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.5 Looking into the probability scores with positive reviews \n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Find the top 5 movie reviews in `results_df` with the highest predicted probability of being positive (i.e., where the model is most confident that the review is positive). If you are curious to read these reviews, you can set the pandas column width or using `IPython.display.HTML` [using the tips in this so thread](https://stackoverflow.com/questions/25351968/how-can-i-display-full-non-truncated-dataframe-information-in-html-when-conver/)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Looking into the probability scores with negative reviews \n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Find the top 5 movie reviews in `results_df` with the highest predicted probability of being negative (i.e., where the model is most confident that the review is negative).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.7 Looking at uncertain reviews\n",
    "rubric={accuracy:2}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Find the 5 movie reviews in the test set with the most divided probability of being negative or positive (i.e., where the model is least confident in either review sentiment).\n",
    "\n",
    "What do you think could contribute to the model being confused for how to score a review?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Looking at wrongly predicted reviews\n",
    "rubric={accuracy:1,reasoning:1}\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\">\n",
    "    \n",
    "Examine a review from the test set where our `best_model` is making mistakes, i.e., where the true labels do not match the predicted labels. \n",
    "\n",
    "What do you think could contribute to the model making an incorrect classification for a review?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to Canvas\n",
    "\n",
    "**PLEASE READ: When you are ready to submit your assignment do the following:**\n",
    "\n",
    "- Read through your solutions\n",
    "- **Restart your kernel and clear output and rerun your cells from top to bottom** \n",
    "- Makes sure that none of your code is broken \n",
    "- Convert your notebook to .html format by going to File -> Export Notebook As... -> Export Notebook to HTML\n",
    "- Upload your `.ipynb` file and the `.html` file to Canvas under Assignment1. \n",
    "- **DO NOT** upload any `.csv` files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on finishing Assignment 3!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "bait509",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.986px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
